{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from data_utils import DataIterator\n",
    "\n",
    "max_pool = tf.contrib.keras.layers.GlobalMaxPool1D()\n",
    "\n",
    "\n",
    "class SiameseNet(object):\n",
    "    def __init__(self, config, embeddings):\n",
    "        self.config = config\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def bilstm(self, seq, seq_len):\n",
    "        cell_fw = tf.nn.rnn_cell.LSTMCell(self.config.hidden_size)\n",
    "        cell_bw = tf.nn.rnn_cell.LSTMCell(self.config.hidden_size)\n",
    "        (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, seq, sequence_length=seq_len,\n",
    "                                                                    dtype=tf.float32)\n",
    "        output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "        return output\n",
    "\n",
    "    def lstm(self, seq, seq_len):\n",
    "        cell_fw = tf.nn.rnn_cell.LSTMCell(self.config.hidden_size)\n",
    "        output, state = tf.nn.dynamic_rnn(cell_fw, seq, sequence_length=seq_len, dtype=tf.float32)\n",
    "        return output\n",
    "\n",
    "    def activation(self, x):\n",
    "        assert self.config.fd_activation in [\"sigmoid\", \"relu\", \"tanh\"]\n",
    "        if self.config.fd_activation == \"sigmoid\":\n",
    "            return tf.nn.sigmoid(x)\n",
    "        elif self.config.fd_activation == \"relu\":\n",
    "            return tf.nn.relu(x)\n",
    "        elif self.config.fd_activation == \"tanh\":\n",
    "            return tf.nn.tanh(x)\n",
    "\n",
    "    def build(self):\n",
    "        ### Placeholders\n",
    "        self.q1 = tf.placeholder(tf.int64, shape=[None, None], name=\"question1\")\n",
    "        self.l1 = tf.placeholder(tf.int64, shape=[None], name=\"len1\")\n",
    "\n",
    "        self.q2 = tf.placeholder(tf.int64, shape=[None, None], name=\"question2\")\n",
    "        self.l2 = tf.placeholder(tf.int64, shape=[None], name=\"len2\")\n",
    "\n",
    "        self.y = tf.placeholder(tf.int64, shape=[None], name=\"is_duplicate\")\n",
    "\n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[], name=\"dropout\")\n",
    "        self.lr = tf.placeholder(dtype=tf.float32, shape=[], name=\"lr\")\n",
    "\n",
    "        ### Embedding layer\n",
    "        with tf.variable_scope(\"word_embeddings\") as scope:\n",
    "            _word_embeddings = tf.Variable(self.embeddings, name=\"_word_embeddings\", dtype=tf.float32,\n",
    "                                           trainable=self.config.train_embeddings)\n",
    "            we1 = tf.nn.embedding_lookup(_word_embeddings, self.q1, name=\"q1_embedded\")\n",
    "            we2 = tf.nn.embedding_lookup(_word_embeddings, self.q2, name=\"q2_embedded\")\n",
    "\n",
    "            we1 = tf.nn.dropout(we1, keep_prob=1-self.dropout)\n",
    "            we2 = tf.nn.dropout(we2, keep_prob=1-self.dropout)\n",
    "\n",
    "        ### Shared layer\n",
    "        with tf.variable_scope(\"bilstm\") as scope:\n",
    "            lstm1 = self.bilstm(we1, self.l1)\n",
    "            scope.reuse_variables()\n",
    "            lstm2 = self.bilstm(we2, self.l2)\n",
    "       \n",
    "        ### Max pooling\n",
    "        lstm1_pool = max_pool(lstm1)\n",
    "        lstm2_pool = max_pool(lstm2)\n",
    "\n",
    "        ### Features\n",
    "        flat1 = tf.contrib.layers.flatten(lstm1_pool)\n",
    "        flat2 = tf.contrib.layers.flatten(lstm2_pool)\n",
    "        mult = tf.multiply(flat1, flat2)\n",
    "        diff = tf.abs(tf.subtract(flat1, flat2))\n",
    "        # concat = tf.concat([flat1, flat2, mult, diff], axis=-1)\n",
    "        concat = tf.concat([mult, diff], axis=-1)\n",
    "\n",
    "\n",
    "        ### FC layers\n",
    "        concat_size = int(concat.get_shape()[1])\n",
    "        intermediary_size = 2 + (concat_size - 2) // 2\n",
    "        # intermediary_size = 512\n",
    "\n",
    "        with tf.variable_scope(\"fc1\") as scope:\n",
    "            W1 = tf.Variable(tf.random_normal([concat_size, intermediary_size], stddev=1e-3), name=\"w_fc\")\n",
    "            b1 = tf.Variable(tf.zeros([intermediary_size]), name=\"b_fc\")\n",
    "\n",
    "            preact1 = tf.matmul(concat, W1) + b1\n",
    "            fc1 = self.activation(preact1)\n",
    "            tf.summary.histogram('fc1', fc1)\n",
    "            tf.summary.histogram('W1', W1)\n",
    "            tf.summary.histogram('b1', b1)\n",
    "\n",
    "\n",
    "        with tf.variable_scope(\"fc2\") as scope:\n",
    "            W2 = tf.Variable(tf.random_normal([intermediary_size, 2], stddev=1e-3), name=\"w_fc\")\n",
    "            b2 = tf.Variable(tf.zeros([2]), name=\"b_fc\")\n",
    "\n",
    "            preact2 = tf.matmul(fc1, W2) + b2\n",
    "            # self.fc2 = self.activation(preact2)\n",
    "            self.fc2 = preact2\n",
    "\n",
    "            tf.summary.histogram('fc2', self.fc2)\n",
    "            tf.summary.histogram('W2', W2)\n",
    "            tf.summary.histogram('b2', b2)\n",
    "\n",
    "        ### Loss\n",
    "        self.cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.fc2))\n",
    "\n",
    "        ### Optimizer\n",
    "        with tf.variable_scope(\"train_step\") as scope:\n",
    "            if self.config.lr_method == \"adam\":\n",
    "                optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            elif self.config.lr_method == \"sgd\":\n",
    "                optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "\n",
    "            self.train_step = optimizer.minimize(self.cross_entropy)\n",
    "\n",
    "        ### Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(self.fc2, 1), self.y)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        ### Init\n",
    "        self.init = tf.global_variables_initializer()\n",
    "\n",
    "        ### Summaries\n",
    "        with tf.variable_scope(\"summaries\") as scope:\n",
    "\n",
    "            # train\n",
    "            tf.summary.scalar('cross_entropy', self.cross_entropy)\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "            self.merged = tf.summary.merge_all()\n",
    "\n",
    "            # test\n",
    "            self.acc_value = tf.placeholder_with_default(tf.constant(0.0), shape=())\n",
    "            self.ce_value = tf.placeholder_with_default(tf.constant(0.0), shape=())\n",
    "            acc_summary = tf.summary.scalar('accuracy', self.acc_value)\n",
    "            ce_summary = tf.summary.scalar('cross_entropy', self.ce_value)\n",
    "            self.merged_eval = tf.summary.merge([acc_summary, ce_summary])\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def run_epoch(self, sess, train, dev, test, epoch, lr, test_step=1000):\n",
    "        iterator = DataIterator(train, self.config.batch_size)\n",
    "        nbatches = (iterator.max + self.config.batch_size - 1) // self.config.batch_size\n",
    "        dev_acc = 0\n",
    "\n",
    "        for i in tqdm(range(nbatches)):\n",
    "            q1, q2, l1, l2, y = iterator.__next__()\n",
    "            fd = {self.q1: q1, self.q2: q2, self.l1: l1, self.l2: l2, self.y: y,\n",
    "                  self.dropout: self.config.dropout,\n",
    "                  self.lr: lr}\n",
    "            _, summary, acc = sess.run([self.train_step, self.merged, self.accuracy], feed_dict=fd)\n",
    "\n",
    "            # tensorboard\n",
    "            if i % 10 == 0:\n",
    "                self.train_writer.add_summary(summary, epoch * nbatches + i)\n",
    "\n",
    "            if i % test_step == 0:\n",
    "                summary, dev_acc = self.run_evaluate(sess, dev)\n",
    "                self.dev_writer.add_summary(summary, epoch * nbatches + i)\n",
    "                print(\"Step {}/{}\".format(i, nbatches))\n",
    "                print(\"dev acc {:04.2f}\".format(100 * dev_acc))\n",
    "\n",
    "                summary, test_acc = self.run_evaluate(sess, test)\n",
    "                self.test_writer.add_summary(summary, epoch * nbatches + i)\n",
    "                print(\"test acc {:04.2f}\".format(100 * test_acc))\n",
    "\n",
    "        return dev_acc\n",
    "\n",
    "    def run_evaluate(self, sess, data):\n",
    "        iterator = DataIterator(data, self.config.batch_size)\n",
    "        nbatches = (iterator.max + self.config.batch_size - 1) // self.config.batch_size\n",
    "\n",
    "        accuracy, cross = 0, 0\n",
    "        for i in range(nbatches):\n",
    "            q1, q2, l1, l2, y = iterator.__next__()\n",
    "            fd = {self.q1: q1, self.q2: q2, self.l1: l1, self.l2: l2, self.y: y,\n",
    "                  self.dropout: 0,\n",
    "                  self.lr: 0}\n",
    "            acc, ce = sess.run([self.accuracy, self.cross_entropy], feed_dict=fd)\n",
    "\n",
    "            accuracy += acc * len(q1)\n",
    "            cross += ce * len(q1)\n",
    "\n",
    "        accuracy /= iterator.max\n",
    "        cross /= iterator.max\n",
    "\n",
    "        summary = sess.run(self.merged_eval, feed_dict={self.acc_value: accuracy, self.ce_value: cross})\n",
    "\n",
    "        return summary, accuracy\n",
    "\n",
    "    def train(self, train_data, dev_data, test_data):\n",
    "\n",
    "        best_acc = 0\n",
    "        nepoch_no_improv = 0\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            self.train_writer = tf.summary.FileWriter(self.config.log_path + \"train\", sess.graph)\n",
    "            self.dev_writer = tf.summary.FileWriter(self.config.log_path + \"dev\", sess.graph)\n",
    "            self.test_writer = tf.summary.FileWriter(self.config.log_path + \"test\", sess.graph)\n",
    "\n",
    "            sess.run(self.init)\n",
    "\n",
    "            lr = self.config.lr\n",
    "\n",
    "            print(\"Training in {}\".format(self.config.conf_dir))\n",
    "            for epoch in range(self.config.n_epochs):\n",
    "                print(\"Epoch {}/{} :\".format(epoch + 1, self.config.n_epochs))\n",
    "                dev_acc = self.run_epoch(sess, train_data, dev_data, test_data, epoch, lr)\n",
    "\n",
    "                lr *= self.config.lr_decay\n",
    "\n",
    "                if dev_acc > best_acc:\n",
    "                    nepoch_no_improv = 0\n",
    "                    if not os.path.exists(self.config.model_path):\n",
    "                        os.makedirs(self.config.model_path)\n",
    "                    self.saver.save(sess, self.config.model_path)\n",
    "                    best_acc = dev_acc\n",
    "                    print(\"New best score on dev !\")\n",
    "\n",
    "                else:\n",
    "                    lr /= self.config.lr_divide\n",
    "                    nepoch_no_improv += 1\n",
    "                    if nepoch_no_improv >= self.config.nepochs_no_improv:\n",
    "                        print(\"Early stopping after {} epochs without improvements\".format(nepoch_no_improv))\n",
    "                        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
